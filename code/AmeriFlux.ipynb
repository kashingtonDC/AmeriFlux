{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from datetime import datetime\n",
    "from scipy.signal import savgol_filter\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the directories\n",
    "working_dir = os.getcwd()\n",
    "shp_dir = os.path.join(os.path.split(os.getcwd())[0], \"shape\")\n",
    "files = [os.path.join(shp_dir,x) for x in os.listdir(shp_dir) if \"ca\" in x if \"amflux\" in x if x.endswith(\".csv\")]\n",
    "\n",
    "# Read the ameriflux csv with all the sites\n",
    "df = pd.read_csv(files[0], error_bad_lines=False, sep='\\t')\n",
    "\n",
    "# Convert the sites CSV (prefiltered for CA) to a gdf \n",
    "df.rename(columns={'Longitude (degrees)': 'Lon', 'Latitude (degrees)': 'Lat', 'Site Id': 'id' }, inplace = True)\n",
    "geometry = [Point(xy) for xy in zip(df.Lon, df.Lat)]\n",
    "crs = {'init': 'epsg:4326'}\n",
    "gdf = gp.GeoDataFrame(df, crs=crs, geometry=geometry)\n",
    "gdf['Lat'] = df['Lat']\n",
    "gdf['Lon'] = df['Lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each site ID, and extract the matching zipfile zips to the amflux_ca_data dir \n",
    "# All USA Ameriflux predownloaded in ../amflux_data\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"../amflux_data\")\n",
    "ca_dir = os.path.join(os.getcwd(), \"../amflux_ca_data\")\n",
    "\n",
    "sids = sorted(gdf.id.unique())\n",
    "\n",
    "for i in sids:\n",
    "    file = [os.path.join(data_dir,x ) for x in os.listdir(data_dir) if i in x][0]\n",
    "    os.chdir(ca_dir)\n",
    "    cmd_str = \"tar -xvf {}\".format(file)\n",
    "    print(cmd_str)\n",
    "    os.system(cmd_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and sort the unzipped files\n",
    "ca_files = [os.path.join(ca_dir, x) for x in os.listdir(ca_dir) if x.endswith(\".csv\")]\n",
    "ca_files.sort()\n",
    "# Read as dfs \n",
    "ca_dfs = [pd.read_csv(x, skiprows = 2) for x in ca_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for what we want - latent heat flux and measurement date. And compute the data record len for each site \n",
    "\n",
    "site_lens = {}\n",
    "filt_dfs = []\n",
    "\n",
    "for idx,i in enumerate(ca_dfs[:]):\n",
    "    \n",
    "    start = datetime.strptime(str(i.TIMESTAMP_START[0]), \"%Y%m%d%H%M\")\n",
    "    end = datetime.strptime(str(i.TIMESTAMP_START[-1:].values[0]), \"%Y%m%d%H%M\")\n",
    "    \n",
    "    nyears = round(((end-start).days/ 365.2425), 2)\n",
    "    \n",
    "    site_lens[sids[idx]] = [nyears]\n",
    "    \n",
    "    print(\"Processing {} years of data for site no {}\".format(nyears, sids[idx]))\n",
    "        \n",
    "    startvals = i.TIMESTAMP_START\n",
    "    endvals = i.TIMESTAMP_END\n",
    "\n",
    "    starts = [datetime.strptime(str(j), \"%Y%m%d%H%M\") for j in startvals]    \n",
    "    ends = [datetime.strptime(str(f), \"%Y%m%d%H%M\") for f in endvals]    \n",
    "    \n",
    "    LE_col = [x for x in i.columns if \"LE\" in x if \"P\" not in x if \"TEST\" not in x if \"MO\" not in x][0]\n",
    "\n",
    "    LE = i[LE_col]\n",
    "\n",
    "    filtered = pd.DataFrame([starts,ends,LE]).T\n",
    "    filtered.columns = ['start','end','LE']\n",
    "    filtered['id'] = sids[idx]\n",
    "\n",
    "    filt_dfs.append(filtered)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and format the data \n",
    "cleaned = []\n",
    "\n",
    "for i in filt_dfs:\n",
    "    cdf = i[i['LE'] != -9999] # filter nans\n",
    "    cdf = cdf[cdf['LE'] != 0] # filter absolute zeros\n",
    "    cdf.start = pd.to_datetime(cdf.start) # Make the dates datetime\n",
    "    cdf.set_index(cdf.start, inplace = True) # Set dates as index\n",
    "    cleaned.append(cdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calc monthly sums, \n",
    "\n",
    "Convert W/m^2 to mm like: \n",
    "\n",
    "Conversion of W m-2 to mm of water\n",
    "Unit of LE is W m-2 or J m-2 s-1\n",
    "L = 2.5 MJ kg-1 (latent heat of evaporation of water)\n",
    "m2 = 100 00 cm2\n",
    "Kg = 1000 g /ρ =1000 cm3 (ρ, density of water = 1 g/cm3 )\n",
    "ET is Latent heat energy divided latent heat of vaporization of water: ET = LE/L = J m-2 s-1 /2.5 MJ kg-1\n",
    "= J m-2 s-1 /2.5 MJ kg-1\n",
    "= J kg /2.5 MJ m2 s\n",
    "Substituting for kg of water and m2 to cm2:\n",
    "= J 1000 cm3 /2.5 MJ 10000 cm2 s\n",
    "= J cm3 /2.5 MJ 10 cm2 s\n",
    "= cm /25 M s\n",
    "= mm/2.5 106 s\n",
    "= 0.00 00 00 4 mm/s In 30 minutes,\n",
    "= 1800 * 0.00 00 00 4 mm/s\n",
    "=0.00072 mm/30 min (varies between 0.00072 and 0.00074 depending on the temperature at latent heat of evaporation of water was assumed) Is this not the same.\n",
    "'''\n",
    "\n",
    "\n",
    "monthly = []\n",
    "mdfs = []\n",
    "\n",
    "for i in cleaned:\n",
    "    \n",
    "    cor = i.LE.astype(np.float32) * 0.00072  # Apply the 30min conversion from W/m^2 to mm\n",
    "    sdf = pd.DataFrame([cor.resample(\"MS\").sum()]).T\n",
    "    sdf['id'] = i.id[0]\n",
    "    \n",
    "    mdfs.append(sdf)\n",
    "    \n",
    "    monthly.append(cor.resample(\"MS\").sum()) # sum to get monthly value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mdfs[:]:\n",
    "    plt.plot(i.LE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_means = {}\n",
    "\n",
    "for i in mdfs:\n",
    "    sid = i.id[0]\n",
    "    site_means[sid] = [np.mean(i.LE)]\n",
    "    \n",
    "sitemeans = pd.DataFrame.from_dict(site_means).T\n",
    "sitemeans.columns = ['ET_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sitelens = pd.DataFrame.from_dict(site_lens).T\n",
    "sitelens.columns = ['nyears']\n",
    "\n",
    "merged1 = pd.merge(gdf,sitemeans, left_on = 'id', right_index = True)\n",
    "amgdf = pd.merge(merged1,sitelens, left_on = 'id', right_index = True)\n",
    "\n",
    "amgdf.plot(column = 'ET_mean', markersize = 20, legend = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read or get the SSEBop ET netcdf data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you want to download do this:\n",
    "\n",
    "# Get the bbox of the amflux data \n",
    "minx, miny, maxx, maxy = amgdf.geometry.total_bounds\n",
    "\n",
    "# Get the ssebop data by bbox \n",
    "comstr = '''curl -o ../data/ssebop.nc https://cida.usgs.gov/thredds/ncss/ssebopeta/monthly?var=et&north={}&west={}&east={}&south={}&disableProjSubset=on&horizStride=1&time_start=2000-01-01T00%3A00%3A00Z&time_end=2019-10-01T00%3A00%3A00Z&timeStride=1'''.format(maxy, minx, maxx, miny)\n",
    "\n",
    "# Run following string from command line, and put it in the ssebop_data dir \n",
    "print(comstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our case we already have the data, so read it in and plot a sample \n",
    "\n",
    "files = [os.path.join(\"../ssebop_data\",x) for x in os.listdir(\"../ssebop_data\") if x.endswith(\".nc\")]\n",
    "ds = xr.open_dataset(files[0])\n",
    "\n",
    "fig, ax=plt.subplots(figsize = (12,8))\n",
    "ds['et'][8].plot(ax = ax, cmap = 'Greens')\n",
    "amgdf.plot(column = 'nyears', markersize = 30, legend = True, vmin = 0, ax = ax)\n",
    "plt.title(\"SSEBop ET and CIMIS record length\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_idx(dd, dd_array):\n",
    "    \"\"\"\n",
    "     search for nearest decimal degree in an array of decimal degrees and return the index.\n",
    "     np.argmin returns the indices of minium value along an axis.\n",
    "     so subtract dd from all values in dd_array, take absolute value and find index of minium.\n",
    "    \"\"\"\n",
    "    geo_idx = (np.abs(dd_array - dd)).argmin()\n",
    "    return geo_idx\n",
    "\n",
    "lats = ds.variables['lat'][:]\n",
    "lons = ds.variables['lon'][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the in situ data\n",
    "all_ameriflux_dat = pd.concat(mdfs)\n",
    "\n",
    "# Filter out the months outside the ssebop range\n",
    "ssebop_start = ds.time[0].values\n",
    "ssebop_end = ds.time[-1].values\n",
    "\n",
    "mask = (all_ameriflux_dat.index > ssebop_start) & (all_ameriflux_dat.index <= ssebop_end)\n",
    "dat = all_ameriflux_dat[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = ds.variables['lat'][:]\n",
    "lons = ds.variables['lon'][:]\n",
    "\n",
    "outdfs = []\n",
    "\n",
    "for i in amgdf.id.unique()[:]:\n",
    "\n",
    "    flux_dat = dat[dat.id == i]\n",
    "\n",
    "    # Extract lat / long value from SSEBop \n",
    "    in_lat = np.array(amgdf[amgdf.id==i].Lat)\n",
    "    in_lon = np.array(amgdf[amgdf.id==i].Lon)\n",
    "\n",
    "    lat_idx = geo_idx(in_lat, lats)\n",
    "    lon_idx = geo_idx(in_lon, lons)\n",
    "\n",
    "    # Compile the out data \n",
    "    for j in flux_dat.index[:]:\n",
    "        \n",
    "        ymd = j.strftime('%Y-%m-%d')        \n",
    "        tempdf = pd.DataFrame([i, ymd, flux_dat[flux_dat.index == j].LE.values[0], ds.sel(time=j)['et'][lat_idx, lon_idx].values]).T\n",
    "        tempdf.columns = ['id','date', 'amflux_et', 'ssebop_et']\n",
    "        \n",
    "        outdfs.append(tempdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat(outdfs)\n",
    "\n",
    "data = dataset.dropna()\n",
    "valid = data[~np.isnan(data.ssebop_et.astype(float))]\n",
    "\n",
    "x,y = valid.amflux_et.astype(float), valid.ssebop_et.astype(float)\n",
    "xy = np.vstack([x,y])\n",
    "z = stats.gaussian_kde(xy)(xy)\n",
    "\n",
    "plt.figure(figsize = (7,4))\n",
    "plt.scatter(x, y, c=z, s=10, edgecolor='')\n",
    "plt.plot([0,300],[0,300], label = '1-1 line')\n",
    "\n",
    "plt.title(\"nobs = {}\".format(len(x)))\n",
    "plt.ylabel(\"SSEBop ET (mm)\")\n",
    "plt.xlabel(\"Ameriflux ET (mm)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms and distributions\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.distplot(valid.ssebop_et.astype(float), color = \"red\", label = \"ssebop\")\n",
    "sns.distplot(valid.amflux_et.astype(float), color= \"green\", label = 'Ameriflux')\n",
    "plt.xlabel(\"ET (mm)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For months where ssebop ET is zero, plot the corresponding CIMIS et \n",
    "\n",
    "ssebop_0 = data.loc[data['ssebop_et'] == 0]\n",
    "plt.hist(ssebop_0.amflux_et.astype(float), bins = 30, alpha = 0.3, edgecolor='black')\n",
    "plt.title(\"Fraction of SSEBop zero ET months = {}%\".format(str(round(len(ssebop_0)/len(data),2)*100)))\n",
    "plt.ylabel(\"count\")\n",
    "plt.xlabel(\"residual (mm)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For months where Amflux ET is zero, plot the corresponding SSEBop et \n",
    "\n",
    "cimis_0 = data.loc[data['amflux_et'] <= 0.001]\n",
    "plt.hist(cimis_0.ssebop_et.astype(float), bins = 30, alpha = 0.3, edgecolor='black')\n",
    "plt.title(\"Fraction of Amflux ET months <= 0.0001 mm ET = {}%\".format(str(round(len(cimis_0)/len(data),5)*100)))\n",
    "plt.ylabel(\"count\")\n",
    "plt.xlabel(\"residual (mm)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean monthly error between SSEBop and amflux \n",
    "\n",
    "dataset = pd.concat(outdfs)\n",
    "\n",
    "dataset.date = pd.to_datetime(dataset.date)\n",
    "\n",
    "monthly_error_mean = []\n",
    "monthly_error_std =[]\n",
    "monthly_ssebop_mean = []\n",
    "monthly_ssebop_std = []\n",
    "monthly_amflux_mean = []\n",
    "monthly_amflux_std = []\n",
    "\n",
    "for i in range(1,13):\n",
    "    # Select all rows for month i \n",
    "    mdf = dataset.loc[dataset['date'].dt.month==i]\n",
    "    \n",
    "    # Get amflux / ssebop stats \n",
    "    monthly_ssebop_mean.append(np.nanmean(mdf.ssebop_et))\n",
    "    monthly_ssebop_std.append(np.nanstd(mdf.ssebop_et))\n",
    "    monthly_amflux_mean.append(np.nanmean(mdf.amflux_et))\n",
    "    monthly_amflux_std.append(np.nanstd(mdf.amflux_et))\n",
    "    \n",
    "    mresid = mdf.amflux_et - mdf.ssebop_et\n",
    "    monthly_error_mean.append(np.nanmean(mresid))\n",
    "    monthly_error_std.append(np.nanstd(mresid))\n",
    "\n",
    "# +- 1 std\n",
    "upper = np.array(monthly_error_mean) + np.array(monthly_error_std)\n",
    "lower = np.array(monthly_error_mean) - np.array(monthly_error_std)\n",
    "\n",
    "upper = upper.reshape(upper.shape[0])\n",
    "lower = lower.reshape(lower.shape[0])\n",
    "\n",
    "amflux_upper = np.array(monthly_amflux_mean) + np.array(monthly_amflux_std)\n",
    "amflux_lower = np.array(monthly_amflux_mean) - np.array(monthly_amflux_std)\n",
    "\n",
    "amflux_upper = amflux_upper.reshape(amflux_upper.shape[0])\n",
    "amflux_lower = amflux_lower.reshape(amflux_lower.shape[0])\n",
    "\n",
    "ssebop_upper = np.array(monthly_ssebop_mean) + np.array(monthly_ssebop_std)\n",
    "ssebop_lower = np.array(monthly_ssebop_mean) - np.array(monthly_ssebop_std)\n",
    "\n",
    "\n",
    "plt.plot(monthly_amflux_mean, label = 'mean amflux')\n",
    "plt.fill_between(range(0,12),amflux_lower, amflux_upper, alpha = 0.3)\n",
    "\n",
    "plt.plot(monthly_ssebop_mean, label = 'mean ssebop')\n",
    "plt.fill_between(range(0,12),ssebop_lower, ssebop_upper, alpha = 0.3)\n",
    "\n",
    "plt.plot(monthly_error_mean, label = 'mean error')\n",
    "plt.fill_between(range(0,12),lower, upper, alpha = 0.3)\n",
    "plt.ylabel(\"ET (mm)\")\n",
    "plt.title(\"Monthly mean / std SSEBOP / amflux ET\")\n",
    "plt.xlabel(\"month\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean error for each site \n",
    "dataset = pd.concat(outdfs)\n",
    "\n",
    "site_errs_m = {}\n",
    "site_errs_v = {}\n",
    "\n",
    "for i in dataset.id.unique():\n",
    "    # Select all rows for month i \n",
    "    mdf = dataset.loc[dataset['id']==i]\n",
    "    err = mdf.amflux_et - mdf.ssebop_et\n",
    "    site_errs_m[i] = [np.nanmean(err)]\n",
    "    site_errs_v[i] = [np.nanvar(err)]\n",
    "\n",
    "    \n",
    "err_m_df = pd.DataFrame.from_dict(site_errs_m).T\n",
    "err_v_df = pd.DataFrame.from_dict(site_errs_v).T\n",
    "\n",
    "err_m_df.columns = ['mean_err']\n",
    "err_v_df.columns = ['var_err']\n",
    "\n",
    "err_df = pd.concat([err_m_df, err_v_df], axis = 1)\n",
    "\n",
    "# err_df = err_df[~(err_df['mean_err'] <= -50)]  # Filter one strangely anomalous site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the SSEBop mean ET and plot the errors at each site \n",
    "\n",
    "err_gdf = pd.merge(amgdf, err_df, left_on = \"id\", right_index = True)\n",
    "\n",
    "et_mean = ds['et'].mean(axis = 0) # Spatial means \n",
    "et_mean = et_mean.where(et_mean!=180) # remove nans\n",
    "\n",
    "fig, ax=plt.subplots(figsize = (12,8))\n",
    "et_mean.plot(cmap = 'Greens',ax = ax)\n",
    "err_gdf.plot(column = 'mean_err', markersize = 50, edgecolor = 'black', cmap = 'coolwarm',legend = True, label = \"mean error (mm)\", ax = ax)\n",
    "plt.title('''Mean monthly (2000 - 2018) SSEBop ET (mm) \n",
    "          and mean erorr (Amflux - SSEBop)''', size = 20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Vegetation Abbreviation (IGBP)', y=\"mean_err\", data=err_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Vegetation Abbreviation (IGBP)', y=\"nyears\", data=err_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(err_gdf['Vegetation Abbreviation (IGBP)'].value_counts().index, err_gdf['Vegetation Abbreviation (IGBP)'].value_counts())\n",
    "plt.ylabel(\"N sites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in err_gdf['Vegetation Description (IGBP)'].unique():\n",
    "    lcdf = err_gdf[err_gdf['Vegetation Description (IGBP)'] == i]\n",
    "    print(str(np.sum(lcdf.nyears.values)) + \" years of data for {}\".format(i))\n",
    "    print(\"====\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the stats we wish to evaluate: TODO Put this in RSfuncs \n",
    "\n",
    "def calc_stats(x,y):\n",
    "    \n",
    "    # RMSE\n",
    "    rmse = float(np.sqrt(np.mean((x - y)**2)))\n",
    "\n",
    "    A = np.vstack([x, np.ones(len(x))]).T\n",
    "    solution = np.linalg.lstsq(A, y)\n",
    "\n",
    "    m, c = solution[0]\n",
    "    residuals = solution[1]\n",
    "    \n",
    "    # R^2\n",
    "    r2 = float(1 - residuals / sum((y - y.mean())**2))\n",
    "\n",
    "    # Calc ubRMSE\n",
    "    ubrmse = float(np.sqrt(np.mean(((x - np.mean(y)) - (x - np.mean(x)))**2)))\n",
    "\n",
    "    # Calc Bias \n",
    "    bias = float(np.mean(x - y))\n",
    "    \n",
    "    return rmse, r2,ubrmse,bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = pd.merge(valid, err_gdf, left_on = \"id\", right_on = \"id\")\n",
    "\n",
    "x,y= master['amflux_et'], master['ssebop_et']\n",
    "\n",
    "rmse,r2,ubrmse,bias = calc_stats(x,y)\n",
    "\n",
    "sns.lmplot(height = 4, aspect = 1.5, x=\"amflux_et\", y=\"ssebop_et\", data=master, fit_reg=False, hue='Vegetation Abbreviation (IGBP)', legend = True)\n",
    "plt.plot([0,350],[0,350])\n",
    "plt.title('''\n",
    "\n",
    "        RMSE = {} | $R^2$ = {} | ubrmse = {} | bias = {} |  N = {}\n",
    "\n",
    "        '''.format(round(rmse,2), round(r2,4), round(ubrmse,3), round(bias,3), len(x)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scatter by lc type \n",
    "master = pd.merge(valid, err_gdf, left_on = \"id\", right_on = \"id\")\n",
    "color_labels = master['Vegetation Abbreviation (IGBP)'].unique()\n",
    "\n",
    "# List of RGB triplets\n",
    "rgb_values = sns.color_palette(\"Set2\", 9)\n",
    "\n",
    "# Map label to RGB\n",
    "color_map = dict(zip(color_labels, rgb_values))\n",
    "\n",
    "# Finally use the mapped values\n",
    "# plt.figure(figsize = (10,8))\n",
    "\n",
    "for i in master['Vegetation Abbreviation (IGBP)'].unique():\n",
    "    lcdf = master[master['Vegetation Abbreviation (IGBP)'] == i]\n",
    "    x,y= lcdf['amflux_et'], lcdf['ssebop_et']\n",
    "    \n",
    "    rmse,r2,ubrmse,bias = calc_stats(x,y)\n",
    "    \n",
    "    plt.scatter(x,y, c= [color_map[i]], label = str(i))\n",
    "    plt.xlabel(\"amflux_et\")\n",
    "    plt.ylabel(\"ssebop_et\")\n",
    "    plt.plot([0,np.max(lcdf['amflux_et'])],[0,np.max(lcdf['amflux_et'])], label = '1-1 line')\n",
    "    plt.title('''\n",
    "\n",
    "            RMSE = {} | $R^2$ = {} | ubrmse = {} | bias = {} |  N = {}\n",
    "\n",
    "            '''.format(round(rmse,2), round(r2,4), round(ubrmse,3), round(bias,3), len(x)))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,x in enumerate (master['Vegetation Abbreviation (IGBP)'].unique()):\n",
    "    print(x + \"  =  \" + master['Vegetation Description (IGBP)'].unique()[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Do the CIMIS based bias correction, and compare the same stats as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the sites file and make a gdf \n",
    "sites = pd.read_excel(os.path.join(shp_dir,\"cimis_sites.xlsx\"))\n",
    "sdf = sites.copy()\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(sites.Longitude, sites.Latitude)]\n",
    "crs = {'init': 'epsg:4326'}\n",
    "cgdf = gp.GeoDataFrame(sdf, crs=crs, geometry=geometry)\n",
    "\n",
    "\n",
    "# Get the length of record for each site \n",
    "\n",
    "# Convert \"active to datetime now\"\n",
    "dt = []\n",
    "for i in cgdf['Disconnect']:\n",
    "    if i ==\"Active\":\n",
    "        dt.append(pd.Timestamp.now())\n",
    "    else:\n",
    "        dt.append(pd.to_datetime(i))\n",
    "\n",
    "# Calc time difference \n",
    "cgdf['Disconnect2'] = dt\n",
    "reclen = cgdf.Connect - cgdf.Disconnect2\n",
    "nyears = [float(x.days / 365) for x in reclen]\n",
    "cgdf['nyears'] = (np.array(nyears)**2)**(1/2)\n",
    "\n",
    "# Rename the first column of the gdf \n",
    "gdfclist = [x for x in cgdf.columns]\n",
    "gdfclist[0] = 'id'\n",
    "cgdf.columns = gdfclist\n",
    "\n",
    "# Plot\n",
    "cgdf.plot(column = 'nyears', markersize = 20, legend = True, vmin = 0, vmax = 40)\n",
    "plt.title(\"Cimis record length (years)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the CIMIS readme, they changed up the db schema in 2014\n",
    "\n",
    "# Columns before 2014\n",
    "columns_00_14 = [ 'id', 'date', 'julian_date', 'srad_qc', 'srad', 'avg_soil_temp_qc', 'avg_soil_temp', \n",
    "                 'max_airtemp_qc', 'max_airtemp', 'min_airtemp_qc', 'min_airtemp', 'avg_airtemp_qc', 'avg_airtemp', 'vp_qc','vp', 'avg_windspeed_qc', \n",
    "                 'avg_windspeed', 'precip_qc', 'precip', 'max_rh_qc', 'max_rh', 'min_rh_qc', 'min_rh', 'ref_eto_qc', 'ref_eto', 'avg_rh_qc', 'avg_rh',\n",
    "                 'dew_pt_qc', 'dew_pt', 'wind_run_qc', 'wind_run']\n",
    "\n",
    "\n",
    "# Columns after 2014\n",
    "columns_14_18 = ['id', 'date', 'julian_date', 'ref_eto','ref_eto_qc', 'precip', 'precpi_qc', 'srad', 'srad_qc', 'vp', 'vp_qc', 'max_airtemp', 'max_airtemp_qc', 'min_airtemp', \n",
    " 'min_airtemp_qc', 'avg_airtemp', 'avg_airtemp_qc', 'max_rh', 'max_rh_qc', 'min_rh', 'min_rh_qc', 'avg_rh', 'avg_rh_qc', 'dew_pt', 'dew_pt_qc', 'avg_windspeed',\n",
    "          'avg_windspeed_qc', 'wind_run', 'wind_run_qc', 'avg_soil_temp', 'avg_soil']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cimis Directories\n",
    "os.chdir(working_dir)\n",
    "cimis_data_dir = os.path.join(working_dir, \"../cimis_data\")\n",
    "cimis_processed = os.path.join(working_dir, \"../cimis_ca_data\")\n",
    "\n",
    "# stn_dir = '/Users/aakash/Desktop/CIMIS/dailyStns/'\n",
    "# data_dir = '/Users/aakash/Desktop/CIMIS/data/'\n",
    "\n",
    "# Find the ET data files and unzip\n",
    "zipfiles = [os.path.join(cimis_data_dir,x) for x in os.listdir(cimis_data_dir) if x.endswith(\".zip\")]\n",
    "\n",
    "outdirs = []\n",
    "for i in zipfiles:\n",
    "    print(\"processing \" + os.path.split(i)[1][:-4])\n",
    "    outdirs.append(os.path.join(cimis_processed,os.path.split(i)[1][:-4]))\n",
    "    os.system('''unzip {} -d{}'''.format(i, os.path.join(cimis_processed,os.path.split(i)[1][:-4])))\n",
    "    \n",
    "outdirs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the big loop to extract hte RS data from stations \n",
    "\n",
    "# First read the lats / lons \n",
    "lats = ds.variables['lat'][:]\n",
    "lons = ds.variables['lon'][:]\n",
    "\n",
    "# Main routine \n",
    "all_dfs = []\n",
    "\n",
    "for i in outdirs[:]:\n",
    "    print(\"Processing \" + i)\n",
    "    workingdir = i\n",
    "    year = int(workingdir[-4:])\n",
    "    dirfiles = [os.path.join(workingdir,x) for x in os.listdir(workingdir)]\n",
    "\n",
    "    if year < 2014:\n",
    "        cols = columns_00_14\n",
    "    else:\n",
    "        cols = columns_14_18\n",
    "    \n",
    "    dfs = []\n",
    "    for i in dirfiles:\n",
    "        \n",
    "        # Remove file if empty \n",
    "        if not os.path.getsize(i) > 1 :\n",
    "            os.remove(i)\n",
    "            continue\n",
    "            \n",
    "        d = pd.read_csv(i, header = None)\n",
    "        d.columns = cols\n",
    "        dfs.append(d)\n",
    "    \n",
    "    # Stack and format dfs \n",
    "    mdf = pd.concat(dfs)\n",
    "    mdf.date = pd.to_datetime(mdf.date)\n",
    "    mdf.set_index('date', inplace = True)\n",
    "    \n",
    "    # Convert to float, add nans for missing data\n",
    "    et = []\n",
    "    for i in mdf.ref_eto:\n",
    "        try:\n",
    "            et.append(float(i))\n",
    "        except:\n",
    "            et.append(np.nan)\n",
    "\n",
    "    # add column to df\n",
    "    mdf['ref_eto_f'] = et\n",
    "\n",
    "    # Filter out QC flagged observaitons\n",
    "    if year < 2014:\n",
    "        fin = mdf[mdf.ref_eto_qc == '*']\n",
    "    else:\n",
    "        fin = mdf[mdf.ref_eto_qc == ' ']\n",
    "    \n",
    "    # remove nans \n",
    "    fdf = fin[fin['ref_eto_f'].notna()]\n",
    "    \n",
    "    # Get the final mean monthly et by site \n",
    "    finals = {}\n",
    "    for i in fdf.id.unique():\n",
    "        sdf = fdf[fdf.id == i]\n",
    "        finals[i] = sdf.ref_eto_f.resample(\"MS\").sum()\n",
    "\n",
    "    monthly = pd.DataFrame.from_dict(finals)\n",
    "    \n",
    "    gdf_filt = cgdf[cgdf['id'].isin(np.array(monthly.columns))]\n",
    "\n",
    "    # Extract the SSebop data for the lat/longs corresponding to CIMIS sites \n",
    "    outdfs = []\n",
    "    \n",
    "    print(\"Processing {} CIMIS stations for {} \".format(len(gdf_filt), str(year)))\n",
    "\n",
    "    for i in gdf_filt.id.unique()[:]:\n",
    "\n",
    "        cimis_dat = monthly[i]\n",
    "\n",
    "        # Extract lat / long value from SSEBop \n",
    "        in_lat = np.array(cgdf[cgdf.id==i].Latitude)\n",
    "        in_lon = np.array(cgdf[cgdf.id==i].Longitude)\n",
    "\n",
    "        lat_idx = geo_idx(in_lat, lats)\n",
    "        lon_idx = geo_idx(in_lon, lons)\n",
    "\n",
    "        # Compile the out data \n",
    "        for j in cimis_dat.index:\n",
    "            yr = j.strftime('%Y-%m-%d')\n",
    "            tempdf = pd.DataFrame([i, yr, cimis_dat[j], ds.sel(time=yr)['et'][lat_idx, lon_idx].values]).T\n",
    "            tempdf.columns = ['id','date', 'cimis_et', 'ssebop_et']\n",
    "            outdfs.append(tempdf)\n",
    "    \n",
    "    all_dfs.append(pd.concat(outdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
